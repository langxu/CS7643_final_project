{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c950d9-91fa-45c4-a64b-ef2dc95150de",
   "metadata": {},
   "source": [
    "## AB Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e05aa18-47c5-4e91-8d12-35cebd6d4123",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _imaging: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_test\\lib\\site-packages\\torchvision\\__init__.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_test\\lib\\site-packages\\torchvision\\datasets\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlsun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSUN, LSUNClass\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfolder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFolder, DatasetFolder\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoco\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CocoCaptions, CocoDetection\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_test\\lib\\site-packages\\torchvision\\datasets\\lsun.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisionDataset\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_test\\lib\\site-packages\\PIL\\Image.py:114\u001b[0m\n\u001b[0;32m    105\u001b[0m MAX_IMAGE_PIXELS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _imaging \u001b[38;5;28;01mas\u001b[39;00m core\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __version__ \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(core, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCore version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(core,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPillow version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m         )\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _imaging: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Constants (must match training)\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "SELECTED_CLASSES = ['cat', 'dog']\n",
    "NUM_CLASSES = len(SELECTED_CLASSES)\n",
    "BATCH_SIZE = 64\n",
    "NORMALIZATION_VARIABLES = {\n",
    "    \"mean\": (0.4914, 0.4822, 0.4465),\n",
    "    \"std\": (0.2470, 0.2435, 0.2616)\n",
    "}\n",
    "\n",
    "\n",
    "class FilteredCIFAR10(datasets.CIFAR10):\n",
    "    def __init__(self, root, train=True, transform=None, download=True):\n",
    "        super().__init__(root, train=train, transform=transform, download=download)\n",
    "        selected_class_indices = [CIFAR10_CLASSES.index(c) for c in SELECTED_CLASSES]\n",
    "        mask = np.isin(self.targets, selected_class_indices)\n",
    "        self.data = self.data[mask]\n",
    "        self.targets = [SELECTED_CLASSES.index(CIFAR10_CLASSES[t]) for t in np.array(self.targets)[mask]]\n",
    "        self.classes = SELECTED_CLASSES\n",
    "\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, depth=28, widen_factor=2, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        n_channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
    "        assert (depth - 4) % 6 == 0\n",
    "        n = (depth - 4) // 6\n",
    "        self.conv1 = nn.Conv2d(3, n_channels[0], kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.block1 = self._make_block(n, n_channels[0], n_channels[1], 1, dropout_rate, True)\n",
    "        self.block2 = self._make_block(n, n_channels[1], n_channels[2], 2, dropout_rate)\n",
    "        self.block3 = self._make_block(n, n_channels[2], n_channels[3], 2, dropout_rate)\n",
    "        self.bn1 = nn.BatchNorm2d(n_channels[3], momentum=0.001)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(n_channels[3], num_classes)\n",
    "\n",
    "    def _make_block(self, n, in_planes, out_planes, stride, dropout_rate=0.0, activate_before_residual=False):\n",
    "        layers = []\n",
    "        for i in range(int(n)):\n",
    "            layers.append(BasicBlock(i == 0 and in_planes or out_planes,\n",
    "                                     out_planes,\n",
    "                                     i == 0 and stride or 1,\n",
    "                                     dropout_rate,\n",
    "                                     activate_before_residual))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropout_rate=0.0, activate_before_residual=False):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes, momentum=0.001)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes, momentum=0.001)\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(\n",
    "            in_planes, out_planes, kernel_size=1, stride=stride, padding=0, bias=True) or None\n",
    "        self.activate_before_residual = activate_before_residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut and self.activate_before_residual:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.bn1(x)\n",
    "            out = self.relu1(out)\n",
    "        out = self.conv1(out if self.equalInOut else x)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        if self.dropout_rate > 0:\n",
    "            out = F.dropout(out, p=self.dropout_rate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        shortcut = x if self.equalInOut else self.convShortcut(x)\n",
    "        return torch.add(out, shortcut)\n",
    "\n",
    "\n",
    "def get_normalizer():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**NORMALIZATION_VARIABLES)\n",
    "    ])\n",
    "\n",
    "\n",
    "def evaluate(model, eval_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in eval_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    per_class_accuracy = []\n",
    "    for i, class_name in enumerate(SELECTED_CLASSES):\n",
    "        correct_class = sum((np.array(all_targets) == i) & (np.array(all_preds) == i))\n",
    "        total_class = sum(np.array(all_targets) == i)\n",
    "        accuracy = 100. * correct_class / total_class if total_class > 0 else 0.0\n",
    "        per_class_accuracy.append(accuracy)\n",
    "\n",
    "    return total_loss / total, 100. * correct / total, per_class_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb498191-7546-4560-95d2-2875dd4655f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Suppose this is what the upward pass to the fix match part\n",
    "\n",
    "Will currently using the local dataset as a tempraray work\n",
    "'''\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# CIFAR-10 类别\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "AB_CLASSES = ['cat', 'dog']\n",
    "CD_CLASSES = ['ship', 'truck']\n",
    "\n",
    "# 本地路径\n",
    "data_dir = r\"C:\\Users\\micha\\OneDrive\\Desktop\\Georgia Institute of Technology\\Spring 2025\\CS7643\\Final_Project\\Data\\cifar-10-batches-py\"\n",
    "test_batch_path = os.path.join(data_dir, \"test_batch\")\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        return pickle.load(fo, encoding='bytes')\n",
    "\n",
    "\n",
    "# 加载 test_batch\n",
    "batch = unpickle(test_batch_path)\n",
    "data = batch[b'data']  # shape: (10000, 3072)\n",
    "labels = batch[b'labels']\n",
    "\n",
    "# 转换成图像格式 (N, 32, 32, 3)\n",
    "data = data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 分组函数\n",
    "def split_by_classes(target_classes):\n",
    "    target_indices = [CIFAR10_CLASSES.index(c) for c in target_classes]\n",
    "    mask = np.isin(labels, target_indices)\n",
    "    selected_images = data[mask]\n",
    "    selected_labels = labels[mask]\n",
    "    label_map = {CIFAR10_CLASSES.index(c): i for i, c in enumerate(target_classes)}\n",
    "    mapped_labels = [label_map[l] for l in selected_labels]\n",
    "    return list(selected_images), mapped_labels\n",
    "\n",
    "\n",
    "# 猫/狗组\n",
    "ab_images, ab_labels = split_by_classes(AB_CLASSES)\n",
    "\n",
    "# 船/卡车组\n",
    "cd_images, cd_labels = split_by_classes(CD_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d80d7c-a977-475b-a991-731cd2558d0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _imaging: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNumpyDataset\u001b[39;00m(Dataset):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_test\\lib\\site-packages\\torchvision\\__init__.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_test\\lib\\site-packages\\torchvision\\datasets\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlsun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSUN, LSUNClass\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfolder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFolder, DatasetFolder\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoco\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CocoCaptions, CocoDetection\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_test\\lib\\site-packages\\torchvision\\datasets\\lsun.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisionDataset\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_test\\lib\\site-packages\\PIL\\Image.py:114\u001b[0m\n\u001b[0;32m    105\u001b[0m MAX_IMAGE_PIXELS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _imaging \u001b[38;5;28;01mas\u001b[39;00m core\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __version__ \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(core, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCore version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(core,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPillow version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m         )\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _imaging: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images  # list of np.array (H, W, C)\n",
    "        self.labels = labels  # list of integers\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 转成 PIL.Image 然后应用 transform\n",
    "        img = Image.fromarray(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42f7f509-4515-4e77-b728-f203821576b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalizer():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**NORMALIZATION_VARIABLES)\n",
    "    ])\n",
    "\n",
    "NORMALIZATION_VARIABLES = {\n",
    "    \"mean\": (0.4914, 0.4822, 0.4465),\n",
    "    \"std\": (0.2470, 0.2435, 0.2616)\n",
    "}\n",
    "transform = get_normalizer()  \n",
    "\n",
    "ab_dataset = NumpyDataset(ab_images, ab_labels, transform=transform)\n",
    "ab_loader = DataLoader(ab_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "cd_dataset = NumpyDataset(cd_images, cd_labels, transform=transform)\n",
    "cd_loader = DataLoader(cd_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38c34539-dd7b-42ca-8701-ee830b7ba16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ab_model = WideResNet(num_classes=2).to(device)\n",
    "ab_model.load_state_dict(torch.load('best_model_ema.pth', map_location=device))\n",
    "\n",
    "cd_model = WideResNet(num_classes=2).to(device)\n",
    "cd_model.load_state_dict(torch.load('best_model_ema_shipTruckFL.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009169a-aeec-48d9-af8f-3b71680ea3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, per_class_acc = evaluate(ab_model, ab_loader, device)\n",
    "print(\"AB组评估结果:\")\n",
    "print(f\"Loss: {loss:.4f}, Accuracy: {acc:.2f}%, Per-class: {per_class_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df1d98-f88e-4879-b103-877db6de24a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

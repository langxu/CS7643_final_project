{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tianyi's Model\n",
    "\n",
    "\n",
    "1. Need to update the \"Weights=None\" to \"pretrain = False\" to align with Shuang's code"
   ],
   "id": "51077af46f715da4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T05:15:43.285111Z",
     "start_time": "2025-04-25T05:15:37.048781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "8f22e8bb1fe33a49",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T05:15:43.502273Z",
     "start_time": "2025-04-25T05:15:43.296456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Need to set the Code as root, and add the folder name in front of every model Tianyi used\n",
    "'''\n",
    "from Tianyi_Model.predict import load_trained_model, predict_image_class\n",
    "from Tianyi_Model.data_preparation import get_cifar10_datasets, get_dataloader\n",
    "from Tianyi_Model.train import train_model"
   ],
   "id": "fb723d93565e2169",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T05:15:44.203804Z",
     "start_time": "2025-04-25T05:15:43.953624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "6be156084c767547",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device name: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T05:15:44.812877Z",
     "start_time": "2025-04-25T05:15:44.221270Z"
    }
   },
   "cell_type": "code",
   "source": "model = load_trained_model('./Tianyi_Model/output/resnet18_4cls_64dim_1tm_model.pth', 'resnet18', 64, 4, device)",
   "id": "7df9e5fedfbb02cf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\Anaconda3\\envs\\final_virtual\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\micha\\Anaconda3\\envs\\final_virtual\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from ./Tianyi_Model/output/resnet18_4cls_64dim_1tm_model.pth\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:24:40.392391Z",
     "start_time": "2025-04-26T00:24:37.774051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset, test_dataset = get_cifar10_datasets(data_dir='./Tianyi_Model/data')\n",
    "train_loader, test_loader = get_dataloader(train_dataset, test_dataset, batch_size=64, num_workers=8)"
   ],
   "id": "cc44fb29f10a09c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering and remapping base training dataset...\n",
      "Dataset filtered. New number of samples: 20000\n",
      "Filtering and remapping base testing dataset...\n",
      "Dataset filtered. New number of samples: 4000\n",
      "Triplet Dataset created with 20000 samples (based on base dataset size).\n",
      "Classes found in dataset: [0, 1, 2, 3]\n",
      "Filtered CIFAR-10 Triplet train loader and standard test loader created with 0 workers.\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:24:40.775579Z",
     "start_time": "2025-04-26T00:24:40.548033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "test data and its label\n",
    "'''\n",
    "NEW_CLASS_MAPPING = {\n",
    "    3: 0, # cat -> 0\n",
    "    5: 1, # dog -> 1\n",
    "    8: 2, # ship -> 2\n",
    "    9: 3  # truck -> 3\n",
    "}\n",
    "\n",
    "print(test_dataset.data.shape)\n",
    "print(len(test_dataset.targets))\n",
    "\n"
   ],
   "id": "a6bc886d97730647",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 32, 32, 3)\n",
      "4000\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:41:34.932068Z",
     "start_time": "2025-04-26T00:40:50.676134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Split data into ab_images/cd_images, and ab_labels/cd_labels\n",
    "'''\n",
    "ab_images, cd_images = [], []\n",
    "ab_labels, cd_labels = [], []\n",
    "# this flag the number of image is miss assign to the fix match model\n",
    "ab_incorrect, cd_incorrect = 0,0\n",
    "# denormalize the image back to its original vector\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "for image, label in test_dataset:\n",
    "    pred_label, prob, _= predict_image_class(model, image, device, True)\n",
    "    # denormalize the image, and put the image shape back to [3, 32, 32]\n",
    "    denom_image = (image * torch.tensor(CIFAR10_STD ).view(1, 3, 1, 1) + torch.tensor(CIFAR10_MEAN ).view(1, 3, 1, 1)).squeeze(0).to(device)\n",
    "\n",
    "    if pred_label in [\"cat\",\"dog\"]:\n",
    "        if label not in [0,1]:\n",
    "            ab_incorrect += 1\n",
    "        else:\n",
    "            ab_images.append(denom_image)\n",
    "            label = torch.tensor(label).long().to(device)\n",
    "            ab_labels.append(label)\n",
    "    elif pred_label in [\"ship\",\"truck\"]:\n",
    "        if label not in [2,3]:\n",
    "            cd_incorrect += 1\n",
    "        else:\n",
    "            cd_images.append(denom_image)\n",
    "            # all fix-match model only takes 0 or 1 as labels\n",
    "            label = torch.tensor(label-2).long().to(device)\n",
    "            cd_labels.append(label)"
   ],
   "id": "2cf4d9c15cc8ebb",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:41:35.195795Z",
     "start_time": "2025-04-26T00:41:34.939156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(ab_images), ab_incorrect)\n",
    "print(len(cd_images), cd_incorrect)"
   ],
   "id": "722eb22c662d182",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1966 49\n",
      "1951 34\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:41:35.536741Z",
     "start_time": "2025-04-26T00:41:35.305026Z"
    }
   },
   "cell_type": "code",
   "source": "denom_image.shape",
   "id": "3c5ec12ddc3c90f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Shuang's Model\n",
   "id": "92baf4e7a4c2f2d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:41:35.864312Z",
     "start_time": "2025-04-26T00:41:35.632813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Constants (must match training)\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "SELECTED_CLASSES = ['cat', 'dog'] # the code itself does not matter, just need to make sure the index 0 is cat/ship, 1 is dog/truck\n",
    "#SELECTED_CLASSES = ['ship', 'truck']\n",
    "NUM_CLASSES = len(SELECTED_CLASSES)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, depth=28, widen_factor=2, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        n_channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
    "        assert (depth - 4) % 6 == 0\n",
    "        n = (depth - 4) // 6\n",
    "        self.conv1 = nn.Conv2d(3, n_channels[0], kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.block1 = self._make_block(n, n_channels[0], n_channels[1], 1, dropout_rate, True)\n",
    "        self.block2 = self._make_block(n, n_channels[1], n_channels[2], 2, dropout_rate)\n",
    "        self.block3 = self._make_block(n, n_channels[2], n_channels[3], 2, dropout_rate)\n",
    "        self.bn1 = nn.BatchNorm2d(n_channels[3], momentum=0.001)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(n_channels[3], num_classes)\n",
    "\n",
    "    def _make_block(self, n, in_planes, out_planes, stride, dropout_rate=0.0, activate_before_residual=False):\n",
    "        layers = []\n",
    "        for i in range(int(n)):\n",
    "            layers.append(BasicBlock(i == 0 and in_planes or out_planes,\n",
    "                                     out_planes,\n",
    "                                     i == 0 and stride or 1,\n",
    "                                     dropout_rate,\n",
    "                                     activate_before_residual))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropout_rate=0.0, activate_before_residual=False):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes, momentum=0.001)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes, momentum=0.001)\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(\n",
    "            in_planes, out_planes, kernel_size=1, stride=stride, padding=0, bias=True) or None\n",
    "        self.activate_before_residual = activate_before_residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut and self.activate_before_residual:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.bn1(x)\n",
    "            out = self.relu1(out)\n",
    "        out = self.conv1(out if self.equalInOut else x)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        if self.dropout_rate > 0:\n",
    "            out = F.dropout(out, p=self.dropout_rate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        shortcut = x if self.equalInOut else self.convShortcut(x)\n",
    "        return torch.add(out, shortcut)\n",
    "\n",
    "def evaluate(model, eval_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in eval_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    per_class_accuracy = []\n",
    "    for i, class_name in enumerate(SELECTED_CLASSES):\n",
    "        correct_class = sum((np.array(all_targets) == i) & (np.array(all_preds) == i))\n",
    "        total_class = sum(np.array(all_targets) == i)\n",
    "        accuracy = 100. * correct_class / total_class if total_class > 0 else 0.0\n",
    "        per_class_accuracy.append(accuracy)\n",
    "\n",
    "    return total_loss / total, 100. * correct / total, per_class_accuracy\n"
   ],
   "id": "6a18ac2f73372c0f",
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:41:36.176722Z",
     "start_time": "2025-04-26T00:41:35.975058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n"
   ],
   "id": "84c4116fb86c7f1",
   "outputs": [],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:41:36.502731Z",
     "start_time": "2025-04-26T00:41:36.271700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def get_normalizer():\n",
    "    return transforms.Compose([\n",
    "        transforms.Normalize(**NORMALIZATION_VARIABLES)\n",
    "    ])\n",
    "\n",
    "NORMALIZATION_VARIABLES = {\n",
    "    \"mean\": (0.4914, 0.4822, 0.4465),\n",
    "    \"std\": (0.2470, 0.2435, 0.2616)\n",
    "}\n",
    "transform = get_normalizer()\n",
    "\n",
    "ab_dataset = TensorDataset(ab_images, ab_labels, transform=transform)\n",
    "ab_loader = DataLoader(ab_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "cd_dataset = TensorDataset(cd_images, cd_labels, transform=transform)\n",
    "cd_loader = DataLoader(cd_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ],
   "id": "388150527125879f",
   "outputs": [],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:41:38.851161Z",
     "start_time": "2025-04-26T00:41:36.614043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "ab_model = WideResNet(num_classes=2).to(device)\n",
    "ab_model.load_state_dict(torch.load('best_model_ema.pth', map_location=device))\n",
    "\n",
    "ab_loss, ab_acc, ab_per_class_acc = evaluate(ab_model, ab_loader, device)\n",
    "\n",
    "cd_model = WideResNet(num_classes=2).to(device)\n",
    "cd_model.load_state_dict(torch.load('best_model_ema_shipTruckFL.pth', map_location=device))\n",
    "\n",
    "cd_loss, cd_acc, cd_per_class_acc = evaluate(cd_model, cd_loader, device)\n",
    "\n",
    "# for the final accuracy calculation, we need to find the correct prediction/total iamges, which incluidng miss classified imaged from contrastive model\n",
    "ab_size = len(ab_loader.dataset)\n",
    "cd_size = len(cd_loader.dataset)\n",
    "final_acc = (ab_acc * ab_size + cd_acc * cd_size)/(ab_size + cd_size + ab_incorrect + cd_incorrect)\n",
    "\n",
    "print(\"Final accuracy: {:.2f}%\".format(final_acc))"
   ],
   "id": "36844a6d36356b18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 79.65%\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:41:39.158834Z",
     "start_time": "2025-04-26T00:41:38.944594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ab_size = len(ab_loader.dataset)\n",
    "cd_size = len(cd_loader.dataset)\n",
    "final_acc = (ab_acc * ab_size + cd_acc * cd_size)/(ab_size + cd_size + ab_incorrect + cd_incorrect)\n",
    "final_acc2 = (ab_acc * ab_size + cd_acc * cd_size)/(ab_size + cd_size)\n",
    "print(\"Final accuracy: {:.2f}%\".format(final_acc))\n",
    "print(\"Final accuracy2: {:.2f}%\".format(final_acc2))"
   ],
   "id": "31f7440b343a8b42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 79.65%\n",
      "Final accuracy2: 81.34%\n"
     ]
    }
   ],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:41:39.499774Z",
     "start_time": "2025-04-26T00:41:39.270142Z"
    }
   },
   "cell_type": "code",
   "source": "(ab_acc * ab_size )/(ab_size + ab_incorrect)",
   "id": "6e90dd149342648e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.4789081885856"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:41:39.793849Z",
     "start_time": "2025-04-26T00:41:39.594525Z"
    }
   },
   "cell_type": "code",
   "source": "(cd_acc * cd_size)/(cd_size + cd_incorrect)",
   "id": "7d7fff438a599170",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.97481108312343"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:41:40.135509Z",
     "start_time": "2025-04-26T00:41:39.904606Z"
    }
   },
   "cell_type": "code",
   "source": "print(ab_acc, cd_acc)",
   "id": "802353a294e6a3e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.21057985757884 91.54279856483855\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:41:40.477231Z",
     "start_time": "2025-04-26T00:41:40.246062Z"
    }
   },
   "cell_type": "code",
   "source": "ab_per_class_acc",
   "id": "848bf973165b78e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[67.24845995893224, 75.1008064516129]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T00:41:40.803219Z",
     "start_time": "2025-04-26T00:41:40.574229Z"
    }
   },
   "cell_type": "code",
   "source": "cd_per_class_acc",
   "id": "e60fd94a7f2b61d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[94.23274974253347, 88.87755102040816]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 156
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "646afe62ca9533a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

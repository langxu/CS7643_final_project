{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tianyi's Model\n",
    "\n",
    "\n",
    "1. Need to update the \"Weights=None\" to \"pretrain = False\" to align with Shuang's code"
   ],
   "id": "51077af46f715da4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T19:45:35.565275Z",
     "start_time": "2025-04-24T19:45:32.539417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "8f22e8bb1fe33a49",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T19:48:07.928820Z",
     "start_time": "2025-04-24T19:48:07.760501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Need to set the Code as root, and add the folder name in front of every model Tianyi used\n",
    "'''\n",
    "from Tianyi_Model.predict import load_trained_model, predict_image_class\n",
    "from Tianyi_Model.data_preparation import get_cifar10_datasets, get_dataloader\n",
    "from Tianyi_Model.train import train_model"
   ],
   "id": "fb723d93565e2169",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T19:49:27.637739Z",
     "start_time": "2025-04-24T19:49:27.373090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "6be156084c767547",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:13:20.346319Z",
     "start_time": "2025-04-24T19:50:08.733722Z"
    }
   },
   "cell_type": "code",
   "source": "model = load_trained_model('./Tianyi_Model/output/resnet18_4cls_64dim_1tm_model.pth', 'resnet18', 64, 4, device)",
   "id": "7df9e5fedfbb02cf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\Anaconda3\\envs\\torch_test\\lib\\site-packages\\torch\\cuda\\__init__.py:106: UserWarning: \n",
      "NVIDIA GeForce RTX 3070 Laptop GPU with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 3070 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from ./Tianyi_Model/output/resnet18_4cls_64dim_1tm_model.pth\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:13:34.697986Z",
     "start_time": "2025-04-24T20:13:32.963622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset, test_dataset = get_cifar10_datasets(data_dir='./Tianyi_Model/data')\n",
    "train_loader, test_loader = get_dataloader(train_dataset, test_dataset, batch_size=128, num_workers=8)"
   ],
   "id": "cc44fb29f10a09c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Filtering and remapping base training dataset...\n",
      "Dataset filtered. New number of samples: 20000\n",
      "Filtering and remapping base testing dataset...\n",
      "Dataset filtered. New number of samples: 4000\n",
      "Triplet Dataset created with 20000 samples (based on base dataset size).\n",
      "Classes found in dataset: [0, 1, 2, 3]\n",
      "Filtered CIFAR-10 Triplet train loader and standard test loader created with 8 workers.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:21:43.669918Z",
     "start_time": "2025-04-24T20:21:43.510483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "test data and its label\n",
    "'''\n",
    "NEW_CLASS_MAPPING = {\n",
    "    3: 0, # cat -> 0\n",
    "    5: 1, # dog -> 1\n",
    "    8: 2, # ship -> 2\n",
    "    9: 3  # truck -> 3\n",
    "}\n",
    "\n",
    "print(test_dataset.data.shape)\n",
    "print(len(test_dataset.targets))\n",
    "\n"
   ],
   "id": "a6bc886d97730647",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 32, 32, 3)\n",
      "4000\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T21:46:01.761101Z",
     "start_time": "2025-04-24T21:45:29.096279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Split data into ab_images/cd_images, and ab_labels/cd_labels\n",
    "'''\n",
    "ab_images, cd_images = [], []\n",
    "ab_labels, cd_labels = [], []\n",
    "\n",
    "for image, label in test_dataset:\n",
    "    pred_label, prob, _= predict_image_class(model, image, device, True)\n",
    "\n",
    "    if pred_label in [\"cat\",\"dog\"]:\n",
    "        ab_images.append(image)\n",
    "        ab_labels.append(label)\n",
    "    elif pred_label in [\"ship\",\"truck\"]:\n",
    "        cd_images.append(image)\n",
    "        cd_labels.append(label)"
   ],
   "id": "2cf4d9c15cc8ebb",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T21:46:01.948851Z",
     "start_time": "2025-04-24T21:46:01.769666Z"
    }
   },
   "cell_type": "code",
   "source": "cd_labels[0:5]",
   "id": "e3f5e8303bac24ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 3, 3, 2]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:35:47.402382Z",
     "start_time": "2025-04-24T22:35:46.975973Z"
    }
   },
   "cell_type": "code",
   "source": "len(ab_labels)",
   "id": "1e5caf92682d0a06",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3a98dd6430db8752"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "403114f1db49ce66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Shuang's Model\n",
   "id": "92baf4e7a4c2f2d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:00:43.313363Z",
     "start_time": "2025-04-24T23:00:43.087461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Constants (must match training)\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "SELECTED_CLASSES = ['cat', 'dog']\n",
    "NUM_CLASSES = len(SELECTED_CLASSES)\n",
    "BATCH_SIZE = 64\n",
    "NORMALIZATION_VARIABLES = {\n",
    "    \"mean\": (0.4914, 0.4822, 0.4465),\n",
    "    \"std\": (0.2470, 0.2435, 0.2616)\n",
    "}\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, depth=28, widen_factor=2, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        n_channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
    "        assert (depth - 4) % 6 == 0\n",
    "        n = (depth - 4) // 6\n",
    "        self.conv1 = nn.Conv2d(3, n_channels[0], kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.block1 = self._make_block(n, n_channels[0], n_channels[1], 1, dropout_rate, True)\n",
    "        self.block2 = self._make_block(n, n_channels[1], n_channels[2], 2, dropout_rate)\n",
    "        self.block3 = self._make_block(n, n_channels[2], n_channels[3], 2, dropout_rate)\n",
    "        self.bn1 = nn.BatchNorm2d(n_channels[3], momentum=0.001)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(n_channels[3], num_classes)\n",
    "\n",
    "    def _make_block(self, n, in_planes, out_planes, stride, dropout_rate=0.0, activate_before_residual=False):\n",
    "        layers = []\n",
    "        for i in range(int(n)):\n",
    "            layers.append(BasicBlock(i == 0 and in_planes or out_planes,\n",
    "                                     out_planes,\n",
    "                                     i == 0 and stride or 1,\n",
    "                                     dropout_rate,\n",
    "                                     activate_before_residual))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropout_rate=0.0, activate_before_residual=False):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes, momentum=0.001)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes, momentum=0.001)\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(\n",
    "            in_planes, out_planes, kernel_size=1, stride=stride, padding=0, bias=True) or None\n",
    "        self.activate_before_residual = activate_before_residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut and self.activate_before_residual:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.bn1(x)\n",
    "            out = self.relu1(out)\n",
    "        out = self.conv1(out if self.equalInOut else x)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        if self.dropout_rate > 0:\n",
    "            out = F.dropout(out, p=self.dropout_rate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        shortcut = x if self.equalInOut else self.convShortcut(x)\n",
    "        return torch.add(out, shortcut)\n",
    "\n",
    "\n",
    "def get_normalizer():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**NORMALIZATION_VARIABLES)\n",
    "    ])\n",
    "\n",
    "def evaluate(model, eval_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in eval_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    per_class_accuracy = []\n",
    "    for i, class_name in enumerate(SELECTED_CLASSES):\n",
    "        correct_class = sum((np.array(all_targets) == i) & (np.array(all_preds) == i))\n",
    "        total_class = sum(np.array(all_targets) == i)\n",
    "        accuracy = 100. * correct_class / total_class if total_class > 0 else 0.0\n",
    "        per_class_accuracy.append(accuracy)\n",
    "\n",
    "    return total_loss / total, 100. * correct / total, per_class_accuracy\n"
   ],
   "id": "6a18ac2f73372c0f",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:00:44.892205Z",
     "start_time": "2025-04-24T23:00:44.722081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images  # list of np.array (H, W, C)\n",
    "        self.labels = labels  # list of integers\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 转成 PIL.Image 然后应用 transform\n",
    "        img = Image.fromarray(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ],
   "id": "fcb26d37b906b62d",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:00:46.828792Z",
     "start_time": "2025-04-24T23:00:46.644778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_normalizer():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**NORMALIZATION_VARIABLES)\n",
    "    ])\n",
    "\n",
    "NORMALIZATION_VARIABLES = {\n",
    "    \"mean\": (0.4914, 0.4822, 0.4465),\n",
    "    \"std\": (0.2470, 0.2435, 0.2616)\n",
    "}\n",
    "transform = get_normalizer()\n",
    "\n",
    "ab_dataset = NumpyDataset(ab_images, ab_labels, transform=transform)\n",
    "ab_loader = DataLoader(ab_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "cd_dataset = NumpyDataset(cd_images, cd_labels, transform=transform)\n",
    "cd_loader = DataLoader(cd_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ],
   "id": "1aae0d30f5543f07",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:01:19.040353Z",
     "start_time": "2025-04-24T23:01:18.711625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ab_model = WideResNet(num_classes=2).to(device)\n",
    "ab_model.load_state_dict(torch.load('best_model_ema.pth', map_location=device))\n",
    "\n",
    "cd_model = WideResNet(num_classes=2).to(device)\n",
    "cd_model.load_state_dict(torch.load('best_model_ema_shipTruckFL.pth', map_location=device))"
   ],
   "id": "50febc24e8d01b33",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-24T23:01:45.384963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 假设 ab_loader 和 cd_loader 的长度分别是 AB 数据集和 CD 数据集的样本数量\n",
    "ab_size = len(ab_loader.dataset)\n",
    "\n",
    "# 获取两个模型的准确率\n",
    "ab_loss, ab_acc, ab_per_class_acc = evaluate(ab_model, ab_loader, device)\n",
    "\n",
    "\n",
    "print(\"整体准确率 (加权平均): {:.2f}%\".format(ab_acc))"
   ],
   "id": "f25064de473c6214",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 假设 ab_loader 和 cd_loader 的长度分别是 AB 数据集和 CD 数据集的样本数量\n",
    "ab_size = len(ab_loader.dataset)\n",
    "cd_size = len(cd_loader.dataset)\n",
    "\n",
    "# 获取两个模型的准确率\n",
    "ab_loss, ab_acc, ab_per_class_acc = evaluate(ab_model, ab_loader, device)\n",
    "cd_loss, cd_acc, cd_per_class_acc = evaluate(cd_model, cd_loader, device)\n",
    "\n",
    "# 计算加权平均准确率\n",
    "total_samples = ab_size + cd_size\n",
    "overall_acc = (ab_acc * ab_size + cd_acc * cd_size) / total_samples\n",
    "\n",
    "print(\"整体准确率 (加权平均): {:.2f}%\".format(overall_acc))"
   ],
   "id": "4b09f3b5b7107068"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
